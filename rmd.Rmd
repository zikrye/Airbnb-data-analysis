---
title: "wrangling"
author: "Clare Tang"
date: "2021/11/9"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(dplyr)
library(readr)
library(geojsonR)

library(tidyr)
library(magrittr)
library(ggplot2)
library(sf)
library(tmap)
library(viridis)
library(plotly)
library(devtools)
library(arm)
library(gridExtra)
library(forcats)
library(gstat) 
library(sp)
library(base)
library(ggpubr)
library(sentimentr)
library(tm)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(webshot)
library(reshape2)
library(redres)
```

## Data wrangling

```{r message=FALSE, warning=FALSE, include=FALSE}

# install.packages('R.utils')
listings <- fread("listings.csv.gz")
# listings %>% write_csv("listings.csv")
listings <- as_tibble(listings)
# dim(listings)
# distinct(listings, host_response_time)
# distinct(listings, property_type)
# distinct(listings, room_type)
# distinct(listings, bathrooms)

# REVIEWS <- fread("reviews.csv.gz")
re <- read_csv("reviews.csv.gz")# without strange strings
other_language <- grep("re", iconv(re$comments, "latin1", "ASCII", sub="re"))
# length(other_language)# 106278
review <- re[other_language, ]


neighbourhoods <- read.csv("neighbourhoods.csv")
# View(neighbourhoods)

# install.packages("geojsonR")
# FROM_GeoJson("neighbourhoods.geojson")

reviews <- read.csv("reviews.csv")
# str(reviews)
c_reviews <- reviews %>% count(listing_id)
# max(c_reviews$n)

# calendar <- fread("calendar.csv.gz")
```

```{r message=FALSE, warning=FALSE}

listing <- listings %>% dplyr::select(id, host_id, 
                               host_response_time,
                               host_response_rate,
                               host_is_superhost,
                               # host_total_listings_count, 
                               host_has_profile_pic, 
                               host_identity_verified, 
                               neighbourhood_cleansed, 
                               room_type, 
                               price, 
                               number_of_reviews,
                               review_scores_value, 
                               license, 
                               longitude, 
                               latitude
                               )


listing <- as.data.frame(listing)
# dim(listing) # 3123   15

# summary(is.na(listing)) # 861 na values in 'review_scores_value'
listing <- listing %>% filter(!is.na(review_scores_value))
# dim(listing) # 2262   15

# originally, the host_total_listings_count does not match the unique number of listings each host has
# maybe the reason that some hosts have listings not in boston
# create new host_total_listings_count
listing$host_total_listings_count <- rep(NA, dim(listing)[1])
for(i in 1:dim(listing)[1]){
  listing$host_total_listings_count[i] <- sum(listing$host_id == listing$host_id[i])
}

# number of hosts
length(unique(listing$host_id)) # 1183

sum(listing$host_response_time == "N/A") # 682
sum(listing$host_response_rate == "N/A") # 682

# filter na value
unique <- lapply(listing, unique)
unique$host_response_time # N/A + 3
unique$host_response_rate # N/A
unique$host_is_superhost # t/f
unique$host_has_profile_pic # t/f
unique$host_identity_verified # t/f
length(unique$neighbourhood_cleansed) # 25
unique$room_type # 4
# unique$license
unique$host_total_listings_count

listin <- listing %>% filter(host_response_time != "N/A" &  host_response_rate != "N/A" )
# summary(is.na(listin))

# crerate new variables
listin$host_response_time <- factor(listin$host_response_time, 
                                    levels = c("within an hour", 
                                               "within a few hours",
                                               "within a day", 
                                               "a few days or more"
                                               ))
listin$host_is_superhost <- factor(listin$host_is_superhost, 
                                   levels = c("f", "t"))
listin$host_has_profile_pic <- factor(listin$host_has_profile_pic, 
                                   levels = c("f", "t"))
listin$host_identity_verified <- factor(listin$host_identity_verified, 
                                   levels = c("f", "t"))
listin$room_type <- factor(listin$room_type, 
                                   levels = c("Entire home/apt", "Private room", "Hotel room",  "Shared room"))

listin$license_ornot <- ifelse(listin$license == "", 0, 1)
listin$license_ornot <- factor(listin$license_ornot)

listin$host_response_rate <- as.numeric(gsub("[\\%, ]", "", listin$host_response_rate))
listin$host_response_rate <- listin$host_response_rate/100

listin$price <- as.numeric(gsub("\\$", "", listin$price))
sum(is.na(listin$price))
listin <- listin %>% filter(price != 0, !is.na(price))

dim(listin)

count_hid <- count(listin, host_id)


```

## Basic plot

1. Distribution of the number of listings own by different hosts
    Most hosts own less than 5 listings
2. Number of listings in different Boston neighborhoods
3. Density of reviews of listings in different Boston neighborhoods
4. Number of different type of listings in different Boston neighborhoods

```{r, warning=FALSE}

# distribuiton of host_total_listings_count
ggplot(data = listing, aes(x = host_total_listings_count))+
  geom_histogram(binwidth = 1)

count <- count(listin, neighbourhood_cleansed)
ggplot(count, aes(x = reorder(neighbourhood_cleansed, -n), y = n))+
  geom_bar(stat = "identity")+
  coord_flip()+
  ylab("Number of listings")+
  xlab("Boston neighborhoods")+
  geom_text(aes(label = n), hjust=-0.5, position = "dodge")


# draw the distribution of review scores of 20 hosts
# set.seed(1)
# oh_hid <- sample(unique(listin$host_id), 20, replace = FALSE)
# oh_listin <- listin %>% filter(host_id %in% oh_hid)
# ggplot(oh_listin)+
#   geom_density(alpha = .3)+
#   aes(x = review_scores_value, color = host_id)+
#   facet_wrap(~ host_id)+
#   theme(legend.position = "none")+
#   geom_rug()+
#   xlab("review values")+ 
#   geom_vline(xintercept = mean(oh_listin$review_scores_value), color = "red", lty = 2)


ggplot(listin)+
  geom_density(alpha = .3)+
  aes(x = review_scores_value, color = neighbourhood_cleansed)+
  facet_wrap(~ neighbourhood_cleansed)+
  theme(legend.position = "none")+
  geom_rug()+
  xlab("review values")+ 
  geom_vline(xintercept = mean(listin$review_scores_value), color = "red", lty = 2)



# bar plot: type of listings
roomdf <- listin %>% group_by(neighbourhood_cleansed, room_type) %>% summarize(Freq = n())
total_room <- listin %>% group_by(neighbourhood_cleansed) %>% summarize(sum = n())
ratio_room <- merge(roomdf, total_room, by = "neighbourhood_cleansed")
ratio_room <- ratio_room %>% mutate(ratio = Freq/sum)

ggplot(ratio_room, aes(x = Freq, y = neighbourhood_cleansed, fill = room_type))+
  geom_bar(position = position_dodge(preserve = 'single'), stat = "identity")+
  xlab("Number of rooms")+ ylab("Area")+
  scale_fill_discrete(name = "Room type")+
  ggtitle("Types of room in different neighborhoods")+
  theme(axis.text.x = element_text(angle = 45))

```

  
  
## Boston neighborhoods

1. read boston neighborhoods shape file
2. transfer lisitng dataframe into shape file by using the location of listings (longitude and latitude)
3. maps dot plot of price of listings for different Boston neighborhoods


```{r message=FALSE, warning=FALSE}
boston <- st_read("Boston_Neighborhoods/Boston_Neighborhoods.shp", quiet = TRUE)
epsg_wgs84 <- 4326
# boston %>% st_transform(epsg_wgs84)

sf_listin <- listin %>% st_as_sf(coords = c("longitude", "latitude")) %>% st_set_crs(epsg_wgs84)
print(sf_listin, n = 5)

ggplot()+
  geom_point(data = listin,
             aes(longitude,
                 latitude,
                 color = price, size = .8), alpha = .3)+
  theme(legend.position = "none")


```


## Map of review_scores_value

1. calculate the mean value of reviews score of different Boston neighborhoods and plot
2. boxplot of reviews scores of different Boston neighborhoods

```{r message=FALSE, warning=FALSE}

nopooling_rs <- listin %>% 
  group_by(neighbourhood_cleansed) %>% 
  do(tidy(lm(review_scores_value ~ 1, .)))

rs <- data.frame(Name = count$neighbourhood_cleansed, nopool_rs = nopooling_rs$estimate, stringsAsFactors = FALSE)

np_rs <- left_join(boston, rs, by = "Name")

plot_nopool_rs <- np_rs %>%
  ggplot(aes(fill = nopool_rs, color = nopool_rs))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1, option = "A")+
  scale_color_viridis(direction = -1, option = "A")+
  labs(title = "Review scores", subtitle = "Nopooling by boston neighborhoods")
plot_nopool_rs



ggplot(listin, aes(x = fct_reorder(neighbourhood_cleansed, review_scores_value),
                   y = review_scores_value,
                   color = neighbourhood_cleansed))+
  geom_boxplot()+
  geom_jitter(color = "black", width = .2, size = .5, alpha = .5)+
  coord_flip()+
  theme(legend.position = "none")+
  labs(y = "Review scores value", x = "Boston neighborhoods")+
  scale_x_discrete(position = "top")+
  ggtitle("Boxplot of review scores value")
  

```
https://map-rfun.library.duke.edu/032_thematic_mapping_geom_sf.html



## Mean of price by neighborhood

1. nopooling map of price of listings
2. boxplot of price of lisitngs of different Boston neighborhoods

```{r message=FALSE, warning=FALSE}

mp <- listin %>% group_by(neighbourhood_cleansed) %>%
  summarise_at(vars(price), list(mean_p = mean)) %>%
  mutate(log_p = log(mean_p))
names(mp)[1] <- "Name"
# 
join_p <- boston %>% left_join(mp, by = "Name")
# 
# join_p %>% 
#   ggplot(aes(fill = log_p, color = log_p))+
#   geom_sf()+
#   coord_sf(crs = 5070, datum = NA)+
#   scale_color_viridis(direction = -1, option = "A")+
#   scale_fill_viridis(direction = -1, option = "A")+
#   labs(title = "Average price of listings by boston neighborhoods")
# 
# plot_nopool_p <- join_p %>%
#   ggplot(aes(fill = log_p, color = log_p))+
#   geom_sf()+
#   coord_sf(crs = 5070, datum = NA)+
#   scale_fill_viridis(direction = -1)+
#   scale_color_viridis(direction = -1)+
#   labs(title = "Average listing price", subtitle = "Nopooling by boston neighborhoods")



nopooling_p <- listin %>% 
  group_by(neighbourhood_cleansed) %>% 
  do(tidy(lm(log(price) ~ 1, .)))

p <- data.frame(Name = count$neighbourhood_cleansed, nopool_p = nopooling_p$estimate, stringsAsFactors = FALSE)

np_p <- left_join(boston, p, by = "Name")

plot_nopool_p <- np_p %>%
  ggplot(aes(fill = nopool_p, color = nopool_p))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)+
  labs(title = "Listings price", subtitle = "Nopooling by boston neighborhoods")
plot_nopool_p


ggplot(listin, aes(x = fct_reorder(neighbourhood_cleansed, price), y = price, color = neighbourhood_cleansed))+
  geom_boxplot()+
  geom_jitter(color = "black", width = .2, size = .5, alpha = .5)+
  coord_flip()+
  theme(legend.position = "none")+
  labs(x = "Boston neighborhoods", y = "Listing price")+
  ggtitle("Boxplot of listings price")

```



## Other predictors map

1. points map of review scores value
2. points map of types of listings 
3. points map of number of reviews for each listing
4. points map of number of listings each host own

```{r fig.height=6, message=FALSE, warning=FALSE}
# shapefile
mrv_listin <- sf_listin %>%
 group_by(neighbourhood_cleansed) %>%
 summarise_at(vars(review_scores_value), list(mean_rs = mean)) %>%
 dplyr::select(neighbourhood_cleansed, mean_rs)
names(mrv_listin)[1] <- "Name"

# ggplot()+geom_sf(data = boston)+ geom_sf(data = mrv_listin, aes(color = Name))+
#  theme(legend.position = "none")

# tm_shape(sf_listin) +
#  tm_bubbles(col = "room_type", palette = "YlOrBr", size = .2)+
#  tm_legend(outside = TRUE)

ggplot()+
  geom_sf(data = boston)+ 
  geom_sf(data = sf_listin, aes(color = review_scores_value), size = 2, alpha = .5)+
  scale_color_viridis() +
  guides(size=guide_legend(override.aes = list(color = viridis(1))))+
  ggtitle("Distribution of review scores value")

ggplot()+
  geom_sf(data = boston)+ 
  geom_sf(data = sf_listin, aes(color = room_type), size = 2, alpha = .5)+
  ggtitle("Distribution of types of listings")

ggplot()+
  geom_sf(data = boston)+ 
  geom_sf(data = sf_listin, 
          aes(color = number_of_reviews, size = number_of_reviews), alpha = .5)+
  scale_color_viridis(direction = -1) +
  guides(size=guide_legend(override.aes = list(color = viridis(3))))+
  ggtitle("Where do most reviews come from?", subtitle = "Distribution of number of reviews")
  # theme(legend.position = "none")

ggplot()+
  geom_sf(data = boston)+ 
  geom_sf(data = sf_listin, 
          aes(color = host_total_listings_count, size = host_total_listings_count), alpha = .5)+
  scale_color_viridis(direction = -1, option = "G")+
  guides(size=guide_legend(override.aes = list(color = viridis(5))))+
  ggtitle("Where do hosts own more listings", subtitle = "Distribution of host total listings")


```


## Kriging

1. test the variogram assumptions of the price of listings
2. smooth the data of price of listings
3. present in maps

```{r eval=FALSE, include=FALSE}

spherical_variogram <- function (n, ps, r) function (h) {
  h <- h / r
  n + ps * ifelse(h < 1, 1.5 * h - .5 * h ^ 3, 1)
}

v <- variogram(log(price) ~ 1, sf_listin)
v_fit <- fit.variogram(v, vgm("Sph"))
v_f <- spherical_variogram(v_fit$psill[1], v_fit$psill[2], v_fit$range[2])
# 
# # check variogram and covariance
op <- par(mfrow = c(1, 2))
h <- seq(0, 8, length = 100)
plot(v$dist, v$gamma,  pch = 19, col = "gray",
     xlab = "distance", ylab = "semivariogram")
lines(h, v_f(h))
abline(v = v_fit$range[2], col = "gray")
plot(h, sum(v_fit$psill) - v_f(h), type = "l",
     xlab = "distance", ylab = "covariogram",
     ylim = c(0, sum(v_fit$psill)))
points(0, sum(v_fit$psill), pch = 19)
abline(v = v_fit$range[2], col = "gray")
par(op)



chol_solve <- function (C, v) backsolve(C, backsolve(C, v, transpose = TRUE))

kriging_smooth_spherical <- function (formula, data, ...) {
  v <- variogram(formula, data)
  v_fit <- fit.variogram(v, vgm("Sph", ...))
  v_f <- spherical_variogram(v_fit$psill[1], v_fit$psill[2], v_fit$range[2])
  Sigma <- v_f(as.matrix(dist(coordinates(data)))) # semivariogram
  Sigma <- sum(v_fit$psill) - Sigma # prior variance
  tau2 <- v_fit$psill[1] # residual variance
  C <- chol(tau2 * diag(nrow(data)) + Sigma)
  y <- model.frame(formula, data)[, 1] # response
  x <- model.matrix(formula, data)
  # generalized least squares:
  beta <- coef(lm.fit(backsolve(C, x, transpose = TRUE),
                      backsolve(C, y, transpose = TRUE))) # prior mean
  Sigma_inv <- chol2inv(chol(Sigma))
  C <- chol(Sigma_inv + diag(nrow(data)) / tau2)
  # posterior mean (smoother):
  mu <- drop(chol_solve(C, y / tau2 + Sigma_inv %*% x %*% beta))
  list(smooth = mu, prior_mean = beta)
}


tract2<-st_centroid(join_p) #Center the polygon
tract2 = na.omit(tract2)
#tract2$a = exp(tract2$Asthma)
tract2$a = tract2$log_p #The distribution is un-normal, so we use the log transformation here.
breaks <- seq(4.4, 6, by = .1)
tmap_arrange(
  tm_shape(tract2) +
  tm_bubbles(col = "log_p", palette = "-RdYlBu", size = .3, breaks = breaks))
library(purrr)
tract3 <- tract2 %>%
    mutate(x = unlist(map(tract2$geometry,1)),
           y = unlist(map(tract2$geometry,2)))
# tract3
tract4 <- tract3 %>% st_sf() %>% as_Spatial()


ks <- kriging_smooth_spherical(log_p ~ 1, tract4)
y <- tract4$a
op <- par(mfrow = c(1, 2))
plot(ks$smooth, y); abline(0, 1, col = "red")
plot(ks$smooth, type = "l", ylab = "y")
points(y, pch = 19, col = "gray")
abline(h = ks$prior_mean)
par(op)
tract2$smooth <- ks$smooth

tmap_mode("plot")
tmap_arrange(
  tm_shape(tract4) +
    tm_bubbles(col = "log_p", palette = "-RdYlBu", size = .3, breaks = breaks))


# smoothed map comparison
smooth_p <- data.frame(Name = count$neighbourhood_cleansed, 
                    log_p = tract2$a, 
                    smooth = tract2$smooth, 
                    stringsAsFactors = FALSE)

smooth_p <- left_join(boston, smooth_p, by = "Name")

plot_original <- 
  smooth_p %>% 
  ggplot(aes(fill = log_p, color = log_p))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)

plot_smooth <- 
  smooth_p %>% 
  ggplot(aes(fill = smooth, color = smooth))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)

ggarrange(plot_original, plot_smooth, common.legend = TRUE)

```







## Use census tracts to get more block units

this part I am trying to use Boston tracts instead of neighborhoods to divide Boston into more areas, to get more data on variogram

```{r message=FALSE, warning=FALSE}

tracts <- st_read("Census2020_Tracts/Census2020_tracts.shp")

ggplot()+
  geom_sf(data = tracts)+ 
  geom_sf(data = sf_listin, aes(color = review_scores_value), size = 1, alpha = .5)+
  scale_color_viridis() +
  guides(size=guide_legend(override.aes = list(color = viridis(1))))+
  ggtitle("Distribution of review scores value")


# coordinates(listin) <- ~longitude+ latitude


# st_crs(sf_listin) <- st_crs(tracts)
# st_join(tracts, sf_listin)

library(tigris)
# coord <- data.frame(lat = listin$latitude, long = listin$longitude)
# coord$census_code <- apply(coord, 1, 
#                                function(row) call_geolocator_latlon(row['lat'], row['long']))
# coord$GEOID20 <- substr(coord$census_code, start = 1, stop = 11)
# coord$id <- listin$id
# coord$price <- listin$price
# 
# write.csv(coord, 'coord.csv')
coord <- read.csv("coord.csv")

with_code <- left_join(sf_listin, coord, by = "id")

try <- with_code %>% 
  group_by(GEOID20) %>% 
  do(tidy(lm(review_scores_value ~ 1, .)))

try3 <- coord %>% count(GEOID20)

try1 <- data.frame(GEOID20 = try3$GEOID20, 
                   nopool_rs = try$estimate, stringsAsFactors = FALSE)
try1$GEOID20 <- as.character(try1$GEOID20)

try2 <- left_join(tracts, try1, by = "GEOID20")

try2 %>%
  ggplot(aes(fill = nopool_rs, color = nopool_rs))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)+
  labs(title = "Review scores", subtitle = "Nopooling by boston neighborhoods")

```


```{r}

spherical_variogram <- function (n, ps, r) function (h) {
  h <- h / r
  n + ps * ifelse(h < 1, 1.5 * h - .5 * h ^ 3, 1)
}

chol_solve <- function (C, v) backsolve(C, backsolve(C, v, transpose = TRUE))

kriging_smooth_spherical <- function (formula, data, ...) {
  v <- variogram(formula, data)
  v_fit <- fit.variogram(v, vgm("Sph", ...))
  v_f <- spherical_variogram(v_fit$psill[1], v_fit$psill[2], v_fit$range[2])
  Sigma <- v_f(as.matrix(dist(coordinates(data)))) # semivariogram
  Sigma <- sum(v_fit$psill) - Sigma # prior variance
  tau2 <- v_fit$psill[1] # residual variance
  C <- chol(tau2 * diag(nrow(data)) + Sigma)
  y <- model.frame(formula, data)[, 1] # response
  x <- model.matrix(formula, data)
  # generalized least squares:
  beta <- coef(lm.fit(backsolve(C, x, transpose = TRUE),
                      backsolve(C, y, transpose = TRUE))) # prior mean
  Sigma_inv <- chol2inv(chol(Sigma))
  C <- chol(Sigma_inv + diag(nrow(data)) / tau2)
  # posterior mean (smoother):
  mu <- drop(chol_solve(C, y / tau2 + Sigma_inv %*% x %*% beta))
  list(smooth = mu, prior_mean = beta)
}

v <- variogram(log(price.x) ~ 1, with_code)
v_fit <- fit.variogram(v, vgm("Sph"))
v_f <- spherical_variogram(v_fit$psill[1], v_fit$psill[2], v_fit$range[2])
# 
# # check variogram and covariance
op <- par(mfrow = c(1, 2))
h <- seq(0, 8, length = 100)
plot(v$dist, v$gamma,  pch = 19, col = "gray",
     xlab = "distance", ylab = "semivariogram")
lines(h, v_f(h))
abline(v = v_fit$range[2], col = "gray")
plot(h, sum(v_fit$psill) - v_f(h), type = "l",
     xlab = "distance", ylab = "covariogram",
     ylim = c(0, sum(v_fit$psill)))
points(0, sum(v_fit$psill), pch = 19)
abline(v = v_fit$range[2], col = "gray")
par(op)

mean_p_tracts <- coord %>%
 group_by(GEOID20) %>%
 summarise_at(vars(price), list(mean_p = mean)) %>%
 dplyr::select(GEOID20, mean_p)
mean_p_tracts$GEOID20 <- as.character(mean_p_tracts$GEOID20)

join_tracts <- left_join(tracts, mean_p_tracts, by = "GEOID20")

tract_2<-st_centroid(join_tracts) #Center the polygon
tract_2 = na.omit(tract_2)
tract_2$a = log(tract_2$mean_p) #The distribution is un-normal, so we use the log transformation here.
# breaks <- seq(4.4, 6, by = .1)
tmap_arrange(
  tm_shape(tract_2) +
  tm_bubbles(col = "a", palette = "-RdYlBu", size = .3))
library(purrr)
tract_3 <- tract_2 %>%
    mutate(x = unlist(map(tract_2$geometry,1)),
           y = unlist(map(tract_2$geometry,2)))
# tract_3
tract_4 <- tract_3 %>% st_sf() %>% as_Spatial()


k_s <- kriging_smooth_spherical(a ~ 1, tract_4)
y <- tract_4$a
op <- par(mfrow = c(1, 2))
plot(k_s$smooth, y); abline(0, 1, col = "red")
plot(k_s$smooth, type = "l", ylab = "y")
points(y, pch = 19, col = "gray")
abline(h = k_s$prior_mean)
par(op)
tract_2$smooth <- k_s$smooth

tmap_mode("plot")
tmap_arrange(
  tm_shape(tract_4) +
    tm_bubbles(col = "a", palette = "-RdYlBu", size = .3))


# smoothed map comparison
smooth_p_t <- data.frame(GEOID20 = tract_2$GEOID20, 
                    log_p = tract_2$a, 
                    smooth = tract_2$smooth, 
                    stringsAsFactors = FALSE)

smooth_p_t <- left_join(tracts, smooth_p_t, by = "GEOID20")

plot_original_t <- 
  smooth_p_t %>% 
  ggplot(aes(fill = log_p, color = log_p))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)

plot_smooth_t <- 
  smooth_p_t %>% 
  ggplot(aes(fill = smooth, color = smooth))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)

ggarrange(plot_original_t, plot_smooth_t, common.legend = TRUE)


```







## Text mining (review's sentiment analysis)

1. load the dataframe containing the reviews text of listings
2. tidy the content of text: remove numbers, stop words (remove words without true meanings), convert each reviews text to one sentence so that sentiment analysis of each review can be analyzed
3. ten most positive and negative words used by customers in reviews
4. wordcloud of positive and negative words in reviews


```{r}

# word sentiment analysis
re$comments <- removeNumbers(re$comments)
tidy <- re %>% 
  unnest_tokens(word, comments)

tidy <- tidy %>% 
  anti_join(stop_words) %>% 
  dplyr::select(listing_id, word)

# table of word count
tidy %<>% 
  filter(word != "br")

tidy %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 10000) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word))+
  geom_col()+
  labs(y = NULL)+
  geom_text(aes(label = n), hjust=-0.5, position = "dodge")
  

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

id_nb <- listin %>% group_by(id, neighbourhood_cleansed) %>% dplyr::select(id, neighbourhood_cleansed)
names(id_nb)[1] <- "listing_id"
length(unique(tidy$listing_id)) # 2269

tidy_nb <- merge(tidy, id_nb, by = "listing_id")
dim(tidy_nb)

bing_word_counts <- tidy_nb %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n), hjust=-0.5, position = "dodge")+
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

tidy_nb %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors=brewer.pal(8, "Dark2")))

# positive and negative wordcloud
po_word_counts <- bing_word_counts %>% filter(sentiment == "positive") %>% dplyr::select(word, n)
ne_word_counts <- bing_word_counts %>% filter(sentiment == "negative") %>% dplyr::select(word, n)

wordcloud2(po_word_counts, size=1.6, color='random-dark')
po_cloud <- wordcloud2(po_word_counts, size = 1, minRotation = -0.52, maxRotation = -0.52, rotateRatio = 2)

wordcloud2(ne_word_counts, size=1.6, color='random-dark')
ne_cloud <- wordcloud2(ne_word_counts, size = 1, minRotation = -0.52, maxRotation = -0.52, rotateRatio = 2)

# save image
# webshot::install_phantomjs()
# library("htmlwidgets")
# saveWidget(po_cloud,"po_cloud.html",selfcontained = F)
# webshot("po_cloud.html", "po_cloud.png", delay =5, vwidth = 650, vheight=650)

tidy_nb %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)







# sentence sentiment analysis

review$raword <- removePunctuation(review$comments)
review$raword <- paste(review$raword, ". ")

# dat4$raword <- gsub('\\.', '', dat4$comments)
# dat4$raword <- tolower(dat4$raword)
# install.packages("splus2R")
# library(splus2R)
# lowerCase(REVIEWS$raword)

# comments <- review$raword %>% 
#   get_sentences() %>% 
#    sentiment() %>% 
#    mutate(polarity_level = ifelse(sentiment < 0, "Negative",
#                                   ifelse(sentiment > 0,
#                                          "Positive","Neutral")))

# write.csv(comments, 'comments.csv')
comments <- read.csv("comments.csv")

comments %>% filter(polarity_level != "Neutral") %>% 
  ggplot() + geom_histogram(aes(x = sentiment), binwidth = .1, bins = 30)

# density plot
comments %>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% #View()
  ggplot() + geom_density(aes(ave_sentiment))

```


## Model fitting

log(price) and neighborhoods

1. complete pooling model
2. unpooled model
3. partial pooling model
4. plot comparison of unpooled and partial pooling

```{r message=FALSE, warning=FALSE}

# listin group by neighborhoods
pooled <- lm(log(price) ~ 1, data = listin)
display(pooled)

# mrv
unpooled <- lm(log(price) ~ factor(neighbourhood_cleansed) -1, data = listin)
display(unpooled)
coefplot(unpooled)


partial <- lmer(log(price) ~ 1+ (1 | neighbourhood_cleansed), 
                data = listin)
display(partial)
head(coef(partial)$neighbourhood_cleansed)
head(ranef(partial)$neighbourhood_cleansed)


# prediction for partial pooling
p_pred <- predict(partial, 
                  newdata = data.frame(neighbourhood_cleansed = mp$Name))
length(p_pred) # 23


tdp2 <- data.frame(Name = mp$Name, 
                   pre_p = p_pred, 
                   stringsAsFactors = FALSE)
# head(tdp2)
# dim(tdp2) # 23 2
all_join <- left_join(join_p, tdp2, by = "Name")
dim(all_join)


# prediction for complete pooling
cpred <- predict(pooled, newdata = data.frame(1))


# unpooled map
ggplot(data = all_join) + 
  geom_sf(aes(fill =mean_p, color = mean_p)) + 
  # scale_fill_gradient(limits = c(min,max), na.value = NA)+
  ggtitle("no pooling")


plot_partial <- 
  all_join %>% 
  ggplot(aes(fill = pre_p, color = pre_p))+
  geom_sf()+
  coord_sf(crs = 5070, datum = NA)+
  scale_fill_viridis(direction = -1)+
  scale_color_viridis(direction = -1)+
  # scale_fill_gradientn(colors = viridis_pal()(9), limits=c(min, max), 
                       # na.value = "grey50")+
  labs(title = "Listing price", subtitle = "Partial pooling by boston neighborhoods")

# grid.arrange(plot_nopool_p, plot_partial, ncol = 2)
ggarrange(plot_nopool_p, plot_partial, common.legend = TRUE)



```


## Other models

1. model filtering out insignificant predictors
2. include different slope
3. residual plot

```{r message=FALSE, warning=FALSE}

# density of superhost&review score value
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
n = 2
cols = gg_color_hue(n)

# ggplot(listin)+
#   geom_density(aes(x = review_scores_value, color = factor(host_is_superhost)))+
#   xlab("Review scores value")+
#   ggtitle("Density of review scores value")+
#   annotate("text", x = 3.9, y = 1, label = "not super hosts",color=cols[1])+
#   annotate("text", x = 4.3, y = 2.5, label = "super hosts",color=cols[2])+
#   theme(legend.position = "none")


partial_2 <- lmer(log(price) ~ 1+ 
                    host_response_time+
                    host_response_rate+
                    host_is_superhost+
                    host_has_profile_pic+
                    host_identity_verified+
                    review_scores_value+
                    room_type+
                    host_total_listings_count+
                    license_ornot+
                    (1 | neighbourhood_cleansed), 
                data = listin)
display(partial_2)
head(coef(partial_2)$neighbourhood_cleansed)
fixef(partial_2)
head(ranef(partial_2)$neighbourhood_cleansed)

# filter not significant predictors
partial_3 <- lmer(log(price) ~ 
                    host_response_time+
                    host_response_rate+
                    host_is_superhost+
                    review_scores_value+
                    room_type+
                    host_total_listings_count+
                    license_ornot+
                    (1 + host_total_listings_count+ host_is_superhost| neighbourhood_cleansed), 
                data = listin)
display(partial_3)
head(coef(partial_3)$neighbourhood_cleansed)
head(ranef(partial_3)$neighbourhood_cleansed)

res_3 <- residuals(partial_3)
res_3 <- as.data.frame(res_3)
ggplot(res_3, aes(res_3))+  geom_histogram(fill = 'blue', alpha = 0.5)
# plot(partial_3)

# redyes redres
# devtools::install_github("goodekat/redres")

# all kinds of residual of model3
rc_resids <- compute_redres(partial_3)
pm_resids <- compute_redres(partial_3, type = "pearson_mar")
sc_resids <- compute_redres(partial_3, type = "std_cond")


plot_redres(partial_2, type = "std_cond")
plot_redres(partial_3, type = "std_cond")

plot_resqq(partial)
plot_resqq(partial_2)
plot_resqq(partial_3)

qqnorm(residuals(partial))
qqnorm(residuals(partial_2))
qqnorm(residuals(partial_3))


anova(partial, partial_2, partial_3, test = "Chisq")

# consider interaction
# library(corrplot)
# library(GGally)
# cor.test(listin$number_of_reviews, listin$review_scores_value)
# ggpairs(listin[, c("price", "review_scores_value")])

# why choose multi-level model
try <- data.frame(his = ifelse(listin$host_is_superhost == "f", 0, 1),
                  p = listin$price, 
                  nc = factor(listin$neighbourhood_cleansed))
count(try, nc)

ggplot(try, aes(x = his, y = log(p), color = nc))+
  geom_point()+
  stat_summary(fun = "mean", geom = "line", alpha = .3)+
  stat_summary(fun = "mean", geom = "line", lty = 2, aes(group = 1), color = "black")+
  theme(legend.position="none")

# lis <- listings[, c("host_is_superhost", "neighbourhood_cleansed", "review_scores_value")]
# lis <- lis %>% filter(review_scores_value!= "NA" & neighbourhood_cleansed == "Allston")
# lis$host_is_superhost <- as.factor(lis$host_is_superhost)
# plot(lis$host_is_superhost, lis$review_scores_value)


```
